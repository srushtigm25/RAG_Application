{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet langchain langchain-community langchain-openai chromadb \n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain community + core imports\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Other Python modules\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbGHn7tbvfliXOOdCpzscuSKffBrx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--cea14857-29f3-4594-8701-1f89ed0fea97-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 18 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='APPLIED COGNITIVE PSYCHOLOGY\\nAppl. Cognit. Psychol. 20: 139–156 (2006)\\nPublished online 31 October 2005 in Wiley InterScience\\n(www.interscience.wiley.com) DOI: 10.1002/acp.1178\\nConsequences of Erudite Vernacular Utilized Irrespective\\nof Necessity: Problems with Using Long Words Needlessly\\nDANIEL M. OPPENHEIMER*\\nPrinceton University, USA\\nSUMMARY\\nMost texts on writing style encourage authors to avoid overly-complex words. However, a majority\\nof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give\\nthe impression of intelligence. This paper explores the extent to which this strategy is effective.\\nExperiments 1–3 manipulate complexity of texts and ﬁnd a negative relationship between complex-\\nity and judged intelligence. This relationship held regardless of the quality of the original essay, and\\nirrespective of the participants’ prior expectations of essay quality. The negative impact of\\ncomplexity was mediated by processing ﬂuency. Experiment 4 directly manipulated ﬂuency and\\nfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5\\ninvestigated discounting of ﬂuency. When obvious causes for low ﬂuency exist that are not relevant\\nto the judgement at hand, people reduce their reliance on ﬂuency as a cue; in fact, in an effort not to\\nbe inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\\ndirection. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\\nWhen it comes to writing, most experts agree that clarity, simplicity and parsimony are\\nideals that authors should strive for. In their classic manual of style, Strunk and White\\n(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\\nsubmission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\\nsimply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\\nsentences with simple common words are usually best.’\\nHowever, most of us can likely recall having read papers, either by colleagues or\\nstudents, in which the author appears to be deliberately using overly complex words.\\nExperience suggests that the experts’ advice contrasts with prevailing wisdom on how to\\nsound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\\nabout their writing habits, most of them admitted that they had made their writing more\\ncomplex in order to appear smarter. For example, when asked, ‘Have you ever changed the\\nwords in an academic essay to make the essay sound more valid or intelligent by using\\ncomplicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\\nto choose words that are more complex to give the impression that the content is more\\nvalid or intelligent?’\\nCopyright # 2005 John Wiley & Sons, Ltd.\\n*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room\\n2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='There are many plausible reasons that the use of million-dollar words would lead\\nreaders to believe that an author is smart. Intelligence and large vocabularies are positively\\ncorrelated (Spearman, 1904). Therefore, by displaying a large vocabulary, one may be\\nproviding cues that he or she is intelligent as well. Secondly, writers are assumed to be\\nconforming to the Gricean maxim of manner, ‘avoid obscurity of expression’ (Grice,\\n1975). If authors are believed to be writing as simply as possible, but a text is nonetheless\\ncomplex, a reader might believe that the ideas expressed in that text are also complex,\\ndefying all attempts to simplify the language. Further, individuals forced to struggle\\nthrough a complex text might experience dissonance if they believe that the ideas being\\nconveyed are simple (Festinger, 1957). Thus, individuals might be motivated to perceive a\\ndifﬁcult text as being more worthwhile, thereby justifying the effort of processing.\\nIndeed, there is some evidence that complex vocabulary can be indicative of a more\\nintelligent author. For example, Penneba ker and King (1999) have shown that the\\npercentage of long words used in class as signments positively correlates with SAT\\nscores and exam grades on both multiple cho ice and essay tests. However it is difﬁcult\\nto draw conclusions about the effectiveness of a strategy of complexity from this data.\\nThe study did not look at how readers of the texts containing the long words perceived\\nthe authors’ intelligence. Thus, it is possi ble that although students using complex\\nvocabularies are objectively very knowledgeable, they might nonetheless be perceived\\nas being less so.\\nWhy might we believe that the experts might be correct in recommending simplicity in\\nwriting? One theory that predicts the effectiveness of straightforward writing is that of\\nprocessing ﬂuency. Simpler writing is easier to process, and studies have demonstrated\\nthat processing ﬂuency is associated with a variety of positive dimensions. Fluency leads\\nto higher judgements of truth (Reber & Schwarz, 1999), conﬁdence (Norwick & Epley,\\n2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &\\nJasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,\\nthe effects of ﬂuency are strongest when the ﬂuency is discrepant—when the amount of\\nexperienced ﬂuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, it\\nwould not be surprising if the lower ﬂuency of overly complex texts caused readers to have\\nnegative evaluations of those texts and the associated authors, especially if the complexity\\nwas unnecessary and thus surprising readers with the relative disﬂuency of the text.\\nBoth the experts and prevailing wisdom present plausible views, but which (if either) is\\ncorrect? The present paper provides an empirical investigation of the strategy of complex-\\nity, and ﬁnds such a strategy to be unsuccessful. Five studies demonstrate that the loss of\\nﬂuency due to needless complexity in a text negatively impacts raters’ assessments of the\\ntext’s authors.\\nEXPERIMENT 1\\nExperiment 1 aimed to answer several simple questions. First, does increasing the\\ncomplexity of text succeed in making the author appear more intelligent? Second, to\\nwhat extent does the success of this strategy depend on the quality of the original, simpler\\nwriting? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss of\\nﬂuency? To answer these questions, graduate school admission essays were made more\\ncomplex by substituting some of the original words with their longest applicable thesaurus\\nentries.\\n140 D. M. Oppenheimer\\nCopyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='While word length is not perfectly interchangeable with sentence complexity—for\\nexample, complexity can come from grammatical structure or infrequent words as\\nwell—it is a useful proxy. Using length as a manipulation of complexity allows for a\\nsimple, easily replicable word replacement algorithm. By keeping content constant and\\nvarying the complexity of vocabulary, it was possible to investigate the effectiveness of\\ncomplexity.\\nParticipants and procedure\\nSeventy-one Stanford University undergraduates participated to fulﬁl part of a course\\nrequirement. The survey was included in a packet of unrelated one-page questionnaires.\\nPackets were distributed in class, and participants were given a week to complete the entire\\npacket.\\nStimuli and design\\nSix personal statements for admissions to graduate studies in English Literature were\\ndownloaded from writing improvement websites. The essays varied greatly both in content\\nand quality of writing. Logical excerpts ranging from 138 to 253 words in length were then\\ntaken from each essay. A ‘highly complex’ version of each excerpt was prepared by\\nreplacing every noun, verb and adjective with its longest entry in the Microsoft Word 2000\\nthesaurus. Words that were longer than any thesaurus entry, were not listed in the\\nthesaurus, or for which there was no entry with the same linguistic sense were not\\nreplaced. If two entries were of the same length, the replacement was chosen alphabe-\\ntically. When necessary, minor modiﬁcations were made to the essay to maintain the\\ngrammatical structure of a sentence (e.g. replacing ‘an’ with ‘a’ for replacement words\\nbeginning with consonants). A ‘moderately complex’ version of each excerpt was created\\nusing the same algorithm as above, except replacing only every third applicable word.\\nExamples of the stimuli can be found in the appendix.\\nEach participant received only one excerpt. Participants were informed that the excerpt\\ncame from a personal statement for graduate study in the Stanford English department.\\nThey were instructed to read the passage, decide whether or not to accept the applicant,\\nand rate their conﬁdence in their decision on a 7-point scale. 1 They were then asked how\\ndifﬁcult the passage was to understand, also on a seven-point scale.\\nResults\\nThe data of one participant was discarded due to an illegible answer. Analysis of the\\nmanipulation check showed that more complex texts were more difﬁcult to read. ( x ¼ 2.9,\\n4.0 and 4.3 for simple, moderately complex and highly complex, respectively). These\\ndifferences were reliable, F(2, 68) ¼ 4.46, p < 0.05, Cohen’s f ¼ 0.18. For other analyses,\\nacceptance ratings ( þ1 for accept, /C01 for reject) were multiplied by conﬁdence ratings to\\ncreate a /C07 to 7 scale of admission conﬁdence. Level of complexity had a reliable\\ninﬂuence on admission conﬁdence ratings, F(2, 70) ¼ 2.46, p < 0.05, Cohen’s f ¼ 0.12.\\n1With the exception of the dichotomous admissions decision, all dependent measures reported in this paper are\\nseven point scales ranging from 1 ¼ ‘not at all’ to 7¼ ‘very’.\\nProblems with long words 141\\nCopyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                            chunk_overlap=200,\n",
    "                                            length_function=len,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.4.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langchain-experimental) (1.0.4)\n",
      "Requirement already satisfied: langchain-community<1.0.0,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from langchain-experimental) (0.4.1)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.11/site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.11/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.3.1)\n",
      "Downloading langchain_experimental-0.4.0-py3-none-any.whl (209 kB)\n",
      "Installing collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.4.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U langchain-experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_experimental\n",
    "dir(langchain_experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in ./.venv/lib/python3.11/site-packages (1.0.2)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in ./.venv/lib/python3.11/site-packages (from langchain-openai) (1.0.4)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in ./.venv/lib/python3.11/site-packages (from langchain-openai) (2.7.2)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.42)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./.venv/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in ./.venv/lib/python3.11/site-packages (from scipy) (2.3.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.3.0)\n",
      "Downloading scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.16.3\n",
      "{'score': np.float64(0.775740350604033), 'explanation': 'Cosine similarity between embeddings'}\n"
     ]
    }
   ],
   "source": [
    "# from langchain_experimental.evaluation import load_evaluator\n",
    "\n",
    "# evaluator = load_evaluator(evaluator=\"embedding_distance\", \n",
    "#                             embeddings=embedding_function)\n",
    "\n",
    "# evaluator.evaluate_strings(prediction=\"Amsterdam\", reference=\"coffeeshop\")\n",
    "!pip install -U langchain-openai scipy\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "emb = OpenAIEmbeddings()\n",
    "\n",
    "def embedding_similarity(prediction: str, reference: str) -> float:\n",
    "    \"\"\"Return cosine similarity between two text embeddings (0–1, higher = more similar).\"\"\"\n",
    "    pred_vec = emb.embed_query(prediction)\n",
    "    ref_vec = emb.embed_query(reference)\n",
    "    return 1 - cosine(pred_vec, ref_vec)\n",
    "\n",
    "score = embedding_similarity(\"Paris\", \"coffeeshop\")\n",
    "print({\"score\": score, \"explanation\": \"Cosine similarity between embeddings\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator.evaluate_strings(prediction=\"Paris\", reference=\"coffeeshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # Ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk) \n",
    "\n",
    "    # Create a new Chroma database from the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create vectorstore\n",
    "# vectorstore = create_vectorstore(chunks=chunks, \n",
    "#                                  embedding_function=embedding_function, \n",
    "#                                  vectorstore_path=\"vectorstore_test\")\n",
    "# print(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "    # Generate deterministic UUIDs from the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "\n",
    "    # Keep only unique chunks\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    for chunk, id_ in zip(chunks, ids):\n",
    "        if id_ not in unique_ids:\n",
    "            unique_ids.add(id_)\n",
    "            unique_chunks.append(chunk)\n",
    "\n",
    "    # Create a Chroma vectorstore (auto-persisting)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=unique_chunks,\n",
    "        ids=list(unique_ids),\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=vectorstore_path\n",
    "    )\n",
    "\n",
    "    # ❌ No need for vectorstore.persist() — automatic in new versions\n",
    "    # ✅ Optional manual persist (if you want explicit flush):\n",
    "    # vectorstore._client.persist()\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x11f075490> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"vectorstore_test\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was unnecessary and thus surprising readers with the relative disﬂuency of the text.\n",
      "Both the experts and prevailing wisdom present plausible views, but which (if either) is\n",
      "correct? The present paper ...\n",
      "\n",
      "be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\n",
      "direction. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\n",
      "Wh ...\n",
      "\n",
      "complicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\n",
      "thirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\n",
      "to choose words that are  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: use as retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "docs = retriever.invoke(\"What is this document about?\")\n",
    "for d in docs:\n",
    "    print(d.page_content[:200], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query for relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-chroma\n",
      "  Downloading langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: chromadb<2.0.0,>=1.0.20 in ./.venv/lib/python3.11/site-packages (from langchain-chroma) (1.3.4)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langchain-chroma) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.11/site-packages (from langchain-chroma) (2.3.4)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.12.4)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.38.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (3.11.4)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./.venv/lib/python3.11/site-packages (from chromadb<2.0.0,>=1.0.20->langchain-chroma) (4.25.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-chroma) (0.4.42)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-chroma) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-chroma) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-chroma) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in ./.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-chroma) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-chroma) (2.3.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.28.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./.venv/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma) (25.9.23)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma) (6.33.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.59b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./.venv/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.1.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.20.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (8.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.11/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.3.0)\n",
      "Downloading langchain_chroma-1.0.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: langchain-chroma\n",
      "Successfully installed langchain-chroma-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# # Load vectorstore\n",
    "# # !pip install -U langchain-chroma\n",
    "# from langchain_chroma import Chroma\n",
    "# vectorstore = Chroma(persist_directory=\"vectorstore_chroma\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 1, 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'page_label': '2', 'creator': 'Preview', 'total_pages': 3}, page_content='was unnecessary and thus surprising readers with the relative disﬂuency of the text.\\nBoth the experts and prevailing wisdom present plausible views, but which (if either) is\\ncorrect? The present paper provides an empirical investigation of the strategy of complex-\\nity, and ﬁnds such a strategy to be unsuccessful. Five studies demonstrate that the loss of\\nﬂuency due to needless complexity in a text negatively impacts raters’ assessments of the\\ntext’s authors.\\nEXPERIMENT 1\\nExperiment 1 aimed to answer several simple questions. First, does increasing the\\ncomplexity of text succeed in making the author appear more intelligent? Second, to\\nwhat extent does the success of this strategy depend on the quality of the original, simpler\\nwriting? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss of\\nﬂuency? To answer these questions, graduate school admission essays were made more\\ncomplex by substituting some of the original words with their longest applicable thesaurus\\nentries.\\n140 D. M. Oppenheimer\\nCopyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)'), Document(metadata={'creator': 'Preview', 'author': 'Thu Vu', 'page_label': '1', 'total_pages': 3, 'page': 0, 'moddate': \"D:20240910141854Z00'00'\", 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creationdate': \"D:20240909152042Z00'00'\", 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology'}, page_content='APPLIED COGNITIVE PSYCHOLOGY\\nAppl. Cognit. Psychol. 20: 139–156 (2006)\\nPublished online 31 October 2005 in Wiley InterScience\\n(www.interscience.wiley.com) DOI: 10.1002/acp.1178\\nConsequences of Erudite Vernacular Utilized Irrespective\\nof Necessity: Problems with Using Long Words Needlessly\\nDANIEL M. OPPENHEIMER*\\nPrinceton University, USA\\nSUMMARY\\nMost texts on writing style encourage authors to avoid overly-complex words. However, a majority\\nof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give\\nthe impression of intelligence. This paper explores the extent to which this strategy is effective.\\nExperiments 1–3 manipulate complexity of texts and ﬁnd a negative relationship between complex-\\nity and judged intelligence. This relationship held regardless of the quality of the original essay, and\\nirrespective of the participants’ prior expectations of essay quality. The negative impact of\\ncomplexity was mediated by processing ﬂuency. Experiment 4 directly manipulated ﬂuency and\\nfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5\\ninvestigated discounting of ﬂuency. When obvious causes for low ﬂuency exist that are not relevant\\nto the judgement at hand, people reduce their reliance on ﬂuency as a cue; in fact, in an effort not to\\nbe inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite'), Document(metadata={'author': 'Thu Vu', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'page_label': '1', 'page': 0, 'creationdate': \"D:20240909152042Z00'00'\", 'moddate': \"D:20240910141854Z00'00'\", 'creator': 'Preview', 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'total_pages': 3}, page_content='be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\\ndirection. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\\nWhen it comes to writing, most experts agree that clarity, simplicity and parsimony are\\nideals that authors should strive for. In their classic manual of style, Strunk and White\\n(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\\nsubmission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\\nsimply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\\nsentences with simple common words are usually best.’\\nHowever, most of us can likely recall having read papers, either by colleagues or\\nstudents, in which the author appears to be deliberately using overly complex words.\\nExperience suggests that the experts’ advice contrasts with prevailing wisdom on how to\\nsound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\\nabout their writing habits, most of them admitted that they had made their writing more\\ncomplex in order to appear smarter. For example, when asked, ‘Have you ever changed the\\nwords in an academic essay to make the essay sound more valid or intelligent by using\\ncomplicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus'), Document(metadata={'page': 0, 'total_pages': 3, 'moddate': \"D:20240910141854Z00'00'\", 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'page_label': '1', 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf'}, page_content='complicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\\nto choose words that are more complex to give the impression that the content is more\\nvalid or intelligent?’\\nCopyright # 2005 John Wiley & Sons, Ltd.\\n*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room\\n2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu')]\n"
     ]
    }
   ],
   "source": [
    "# Create retriever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks = retriever.invoke(\"What is the title of the paper?\")\n",
    "relevant_chunks\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer\n",
      "the question. If you don't know the answer, say that you\n",
      "don't know. DON'T MAKE UP ANYTHING.\n",
      "\n",
      "was unnecessary and thus surprising readers with the relative disﬂuency of the text.\n",
      "Both the experts and prevailing wisdom present plausible views, but which (if either) is\n",
      "correct? The present paper provides an empirical investigation of the strategy of complex-\n",
      "ity, and ﬁnds such a strategy to be unsuccessful. Five studies demonstrate that the loss of\n",
      "ﬂuency due to needless complexity in a text negatively impacts raters’ assessments of the\n",
      "text’s authors.\n",
      "EXPERIMENT 1\n",
      "Experiment 1 aimed to answer several simple questions. First, does increasing the\n",
      "complexity of text succeed in making the author appear more intelligent? Second, to\n",
      "what extent does the success of this strategy depend on the quality of the original, simpler\n",
      "writing? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss of\n",
      "ﬂuency? To answer these questions, graduate school admission essays were made more\n",
      "complex by substituting some of the original words with their longest applicable thesaurus\n",
      "entries.\n",
      "140 D. M. Oppenheimer\n",
      "Copyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)\n",
      "\n",
      "---\n",
      "\n",
      "APPLIED COGNITIVE PSYCHOLOGY\n",
      "Appl. Cognit. Psychol. 20: 139–156 (2006)\n",
      "Published online 31 October 2005 in Wiley InterScience\n",
      "(www.interscience.wiley.com) DOI: 10.1002/acp.1178\n",
      "Consequences of Erudite Vernacular Utilized Irrespective\n",
      "of Necessity: Problems with Using Long Words Needlessly\n",
      "DANIEL M. OPPENHEIMER*\n",
      "Princeton University, USA\n",
      "SUMMARY\n",
      "Most texts on writing style encourage authors to avoid overly-complex words. However, a majority\n",
      "of undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give\n",
      "the impression of intelligence. This paper explores the extent to which this strategy is effective.\n",
      "Experiments 1–3 manipulate complexity of texts and ﬁnd a negative relationship between complex-\n",
      "ity and judged intelligence. This relationship held regardless of the quality of the original essay, and\n",
      "irrespective of the participants’ prior expectations of essay quality. The negative impact of\n",
      "complexity was mediated by processing ﬂuency. Experiment 4 directly manipulated ﬂuency and\n",
      "found that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5\n",
      "investigated discounting of ﬂuency. When obvious causes for low ﬂuency exist that are not relevant\n",
      "to the judgement at hand, people reduce their reliance on ﬂuency as a cue; in fact, in an effort not to\n",
      "be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\n",
      "\n",
      "---\n",
      "\n",
      "be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\n",
      "direction. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\n",
      "When it comes to writing, most experts agree that clarity, simplicity and parsimony are\n",
      "ideals that authors should strive for. In their classic manual of style, Strunk and White\n",
      "(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\n",
      "submission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\n",
      "simply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\n",
      "sentences with simple common words are usually best.’\n",
      "However, most of us can likely recall having read papers, either by colleagues or\n",
      "students, in which the author appears to be deliberately using overly complex words.\n",
      "Experience suggests that the experts’ advice contrasts with prevailing wisdom on how to\n",
      "sound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\n",
      "about their writing habits, most of them admitted that they had made their writing more\n",
      "complex in order to appear smarter. For example, when asked, ‘Have you ever changed the\n",
      "words in an academic essay to make the essay sound more valid or intelligent by using\n",
      "complicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\n",
      "thirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\n",
      "\n",
      "---\n",
      "\n",
      "complicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\n",
      "thirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\n",
      "to choose words that are more complex to give the impression that the content is more\n",
      "valid or intelligent?’\n",
      "Copyright # 2005 John Wiley & Sons, Ltd.\n",
      "*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room\n",
      "2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the title of the paper?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate context text\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the title of the paper?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The title of the paper is \"Consequences of Erudite Vernacular Utilized Irrespective of Necessity: Problems with Using Long Words Needlessly.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 1117, 'total_tokens': 1149, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbIvq0uUpxnQkBRpGQVfZ0Ikle416', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7f2c85d2-4353-4937-9c42-5e4a88d1785b-0', usage_metadata={'input_tokens': 1117, 'output_tokens': 32, 'total_tokens': 1149, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The title of the paper is \"Consequences of Erudite Vernacular Utilized Irrespective of Necessity: Problems with Using Long Words Needlessly.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 1111, 'total_tokens': 1143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbIwaKf3wc2Gy3z98pN4c0I50xXTv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--251c4812-0cdd-4830-b668-8b824e7a8692-0', usage_metadata={'input_tokens': 1111, 'output_tokens': 32, 'total_tokens': 1143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What's the title of this paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate structured responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources and reasoning.\"\"\"\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    paper_title: AnswerWithSources\n",
    "    paper_summary: AnswerWithSources\n",
    "    publication_year: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractedInfo(paper_title=AnswerWithSources(answer='Consequences of Erudite Vernacular Utilized Irrespective of Necessity: Problems with Using Long Words Needlessly', sources='SUMMARY\\nMost texts on writing style encourage authors to avoid overly-complex words. However, a majority of undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give the impression of intelligence. This paper explores the extent to which this strategy is effective.', reasoning='The title is referenced directly in the text where it discusses the issue of using complex language unnecessarily.'), paper_summary=AnswerWithSources(answer='The paper investigates the tendency of individuals to use unnecessarily complex vocabulary to appear more intelligent and explores the negative relationship between text complexity and perceived intelligence. Experiments demonstrate that more complex texts are judged to be written by less intelligent authors, and this is mediated by processing fluency.', sources='SUMMARY\\nMost texts on writing style encourage authors to avoid overly-complex words. However, a majority of undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give the impression of intelligence. This paper explores the extent to which this strategy is effective.', reasoning='The summary captures the main findings and thematic focus of the research as described in the retrieved context.'), publication_year=AnswerWithSources(answer='2005', sources='This paper was published in 2006, with the online publication date noted as October 31, 2005.', reasoning=\"The publication year is inferred from the mention of the 2006 issue of 'Applied Cognitive Psychology' and the publication date within that context.\"), paper_authors=AnswerWithSources(answer='Daniel M. Oppenheimer', sources='Copyright # 2005 John Wiley & Sons, Ltd.\\n*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room 2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu', reasoning='The authorship is clearly attributed to Daniel M. Oppenheimer in the correspondence information.'))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm.with_structured_output(ExtractedInfo, strict=True)\n",
    "        )\n",
    "\n",
    "rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform response into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/qx812j190h17vz08lqv3c0r00000gn/T/ipykernel_55691/1998849655.py:2: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  df = pd.DataFrame([structured_response.dict()])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_summary</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>paper_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Consequences of Erudite Vernacular Utilized Ir...</td>\n",
       "      <td>The paper explores how the deliberate use of o...</td>\n",
       "      <td>2005</td>\n",
       "      <td>Daniel M. Oppenheimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>the paper title is \"Consequences of Erudite Ve...</td>\n",
       "      <td>Most texts on writing style encourage authors ...</td>\n",
       "      <td>Published online 31 October 2005 in Wiley Inte...</td>\n",
       "      <td>DANIEL M. OPPENHEIMER* Princeton University, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning</th>\n",
       "      <td>The title is explicitly stated in the context.</td>\n",
       "      <td>The summary is derived from the overarching th...</td>\n",
       "      <td>The publication date is stated in the context.</td>\n",
       "      <td>The authorship is clearly indicated within the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper_title  \\\n",
       "answer     Consequences of Erudite Vernacular Utilized Ir...   \n",
       "source     the paper title is \"Consequences of Erudite Ve...   \n",
       "reasoning     The title is explicitly stated in the context.   \n",
       "\n",
       "                                               paper_summary  \\\n",
       "answer     The paper explores how the deliberate use of o...   \n",
       "source     Most texts on writing style encourage authors ...   \n",
       "reasoning  The summary is derived from the overarching th...   \n",
       "\n",
       "                                            publication_year  \\\n",
       "answer                                                  2005   \n",
       "source     Published online 31 October 2005 in Wiley Inte...   \n",
       "reasoning     The publication date is stated in the context.   \n",
       "\n",
       "                                               paper_authors  \n",
       "answer                                 Daniel M. Oppenheimer  \n",
       "source      DANIEL M. OPPENHEIMER* Princeton University, USA  \n",
       "reasoning  The authorship is clearly indicated within the...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
